# - Gene layer (genes whose protein products are present in tthe PPI network)
# - Gene Ontology layer (biological information to enhance the predictive power of the network)
#
# These layers interact with each other in the following way:
#   - PPI --> Gene (directed: transcription factors regulates the expression of genes)
# - Gene --> PPI (directed: a gene is translated into a protein)
# - GO -- Gene (undirected: the GO terms associated with a gene)
# Retrieve and process data
# Downloaded from stringdb, to run this code, you must download it on your own
PPI_mat = read.delim("../9606.protein.links.detailed.v12.0.txt", sep = " ")
prot_info = read_tsv("../9606.protein.info.v12.0.txt")
# Keep only validated interactions and convert ID name to gene SYMBOL
PPI_mat_clean = PPI_mat %>%
dplyr::filter(experimental >= 700) %>%
dplyr::mutate(protein1_symbol = prot_info$preferred_name[match(protein1, prot_info[[1]])]) %>%
dplyr::mutate(protein2_symbol = prot_info$preferred_name[match(protein2, prot_info[[1]])]) %>%
dplyr::select(protein1_symbol, protein2_symbol)
PPI_mat_clean = PPI_mat_clean[complete.cases(PPI_mat_clean), ]
# Find the proteins in proximity (order = 1) of known targets
PPI_graph = PPI_mat_clean %>%
igraph::graph_from_data_frame()
PPI_to_keep = lapply(igraph::ego(PPI_graph, order = 1, nodes = Known_Target), names) %>% unlist %>% unique
# Wang G, Zhao D, Spring DJ, DePinho RA. Genetics and biology of prostate cancer. Genes Dev. 2018;32(17-18):1105-1140. doi:10.1101/gad.315739.118
Mutated_genes = c("APC", "AR", "ATM", "BRCA1", "BRCA2", "CHD1", "ERF", "ERG", "ETS2", "ETVs", "EZh2", "FOXA1", "IDH1", "KMT2A", "KMT2C", "KMT2D", "MYC", "MYCN", "NCOR1", "NCOR2", "NKX3", "PTEN", "RB1", "SETD2", "SETDB1", "SMAD4", "SMARCA1", "SMARCB1", "SPOP", "TP53")
# Myers JS, von Lersner AK, Robbins CJ, Sang QX. Differentially Expressed Genes and Signature Pathways of Human Prostate Cancer. PLoS One. 2015;10(12):e0145322. Published 2015 Dec 18. doi:10.1371/journal.pone.0145322
Under_expressed_genes = c("WFDC9","DEFB125","EDDM3B","PAEP","SEMG2","PATE4","EDDM3A","CRISP1","PATE1","DEFB127","AQP2","TMEM114","GRXCR1","SPINT3","CLDN2","SULT2A1","SPINK2","POU3F3","LCN1","PATE")
Over_expressed_genes = c("ANKRD30A","FEZF2","C6orf10","FOXG1","GC","VAX1","SSX2","FGB","SLC45A2","SPINK1","HOXC12","SCN1A","LOC284661","TFDP3","B3GNT6","FOXB2","NR2E1","XAGE1E","TBX10")
Signature_for_prostate_cancer = list(Mutated_genes = Mutated_genes,
DEG = c(Under_expressed_genes, Over_expressed_genes))
Signature_for_prostate_cancer = lapply(Signature_for_prostate_cancer, function(x) paste0("Gene_", x))
# Yap, Timothy A et al. “Drug discovery in advanced prostate cancer: translating biology into therapy.” Nature reviews. Drug discovery vol. 15,10 (2016): 699-718. doi:10.1038/nrd.2016.120
Known_Target = c("PARP1","PARP2","FLT1","FLT4","EZH2","AR","ATM","SPINK1","CHD1","SPOP","AURKA","RB1","MYC")
Known_Target_prot = mapIds(org.Hs.eg.db, Known_Target, column = "UNIPROT", keytype = "SYMBOL")
Known_Target_prot = setNames(paste0("Prot_", Known_Target_prot), names(Known_Target_prot))
PPI_to_keep = lapply(igraph::ego(PPI_graph, order = 1, nodes = Known_Target), names) %>% unlist %>% unique
# Sort original PPI to keep only targets and nearest nodes and convert to Uniprot IDs
PPI_mat_clean = PPI_mat_clean %>%
dplyr::filter(protein1_symbol %in% PPI_to_keep | protein2_symbol %in% PPI_to_keep) %>%
dplyr::mutate(protein1_uniprot = paste0("Prot_", mapIds(org.Hs.eg.db, protein1_symbol, "UNIPROT", "SYMBOL"))) %>%
dplyr::mutate(protein2_uniprot = paste0("Prot_", mapIds(org.Hs.eg.db, protein2_symbol, "UNIPROT", "SYMBOL"))) %>%
dplyr::filter(protein1_uniprot != "Prot_NA") %>%
dplyr::filter(protein2_uniprot != "Prot_NA") %>%
dplyr::select(protein1_uniprot, protein2_uniprot, protein1_symbol, protein2_symbol) %>%
unique
# Sort TF to gene information
# packageVersion("dorothea") # ‘1.16.0’
TF_gene_mat = dorothea::entire_database %>%
dplyr::filter(confidence == "A") %>%
dplyr::mutate(tf_uniprot = paste0("Prot_", mapIds(org.Hs.eg.db, tf, "UNIPROT", "SYMBOL"))) %>%
dplyr::mutate(target = paste0("Gene_", target)) %>%
dplyr::filter(tf_uniprot %in% c(PPI_mat_clean$protein1_uniprot, PPI_mat_clean$protein2_uniprot)) %>%
dplyr::select(tf_uniprot, target)
# Sort Gene to GO information
# packageVersion("GOfuncR") # ‘1.24.0’
GO_gene_mat = GOfuncR::get_anno_categories(str_remove(unique(c(PPI_mat_clean$protein1_symbol, PPI_mat_clean$protein2_symbol, TF_gene_mat$target)), "Gene_"))
GO_gene_mat = GO_gene_mat %>%
dplyr::group_by(go_id) %>%
dplyr::mutate(count = n()) %>%
dplyr::filter((count < 20) & (count > 5)) %>% # Keep GO of size between 5 and 20
dplyr::mutate(gene = paste0("Gene_", gene)) %>%
dplyr::mutate(go_id = str_replace(go_id, ":", "_")) %>%
dplyr::select(1,2)
# Remove NAs
PPI_mat_clean = PPI_mat_clean[complete.cases(PPI_mat_clean), ] %>% unique
TF_gene_mat = TF_gene_mat[complete.cases(TF_gene_mat), ] %>% unique
GO_gene_mat = GO_gene_mat[complete.cases(GO_gene_mat), ] %>% unique
# Sort gene to protein information: directed interactions from gene to protein
list_of_proteins = unique(c(PPI_mat_clean$protein1_uniprot, PPI_mat_clean$protein2_uniprot, TF_gene_mat$tf_uniprot))
list_of_proteins_converted_to_symbols = mapIds(org.Hs.eg.db, str_remove(list_of_proteins, "Prot_"), "SYMBOL", "UNIPROT")
PGi_mat = utils::stack(list_of_proteins_converted_to_symbols) %>%
dplyr::mutate(values = paste0("Gene_", values)) %>%
dplyr::mutate(ind = paste0("Prot_", ind)) %>%
dplyr::filter(values != "Gene_NA") %>%
dplyr::filter(ind != "Prot_NA")
# Create the network
PPI_net = PPI_mat_clean[, c(1,2)] %>% igraph::graph_from_data_frame(directed = FALSE)
TFG_net = TF_gene_mat  %>% igraph::graph_from_data_frame(directed = TRUE)
GOG_net = GO_gene_mat %>% igraph::graph_from_data_frame(directed = FALSE)
PGi_net = PGi_mat %>% igraph::graph_from_data_frame(directed = TRUE)
clean_net = function (graph) {
getcompo = components(graph)
getverti = names(getcompo$membership[which(getcompo$membership == which.max(getcompo$csize))])
graph = induced_subgraph(graph, getverti)
return(simplify(graph))
}
A3LNetwork = as.directed(PPI_net, mode = "mutual")+TFG_net+as.directed(GOG_net, mode = "mutual")+PGi_net
A3LNetwork = clean_net(A3LNetwork)
A3LNetwork = igraph::set_vertex_attr(graph = A3LNetwork, name = "type", index = which(str_detect(V(A3LNetwork)$name, "Prot_")), value = "protein")
A3LNetwork = igraph::set_vertex_attr(graph = A3LNetwork, name = "type", index = which(str_detect(V(A3LNetwork)$name, "Gene_")), value = "gene")
A3LNetwork = igraph::set_vertex_attr(graph = A3LNetwork, name = "type", index = which(str_detect(V(A3LNetwork)$name, "GO_"))  , value = "go")
usethis::use_data(A3LNetwork, overwrite = TRUE)
all_proteins = vnames(A3LNetwork, pattern = "Prot_")
all_proteins = stringr::str_subset(igraph::V(A3LNetwork)$name, pattern = "Prot_")
all_proteins %>% summary
all_proteins %>% str
devtools::load_all(".")
# Create clusters from this network based
# Create
all_proteins = vnames(A3LNetwork, pattern = "Prot_")
A3LNetwork_prot =igraph::induced_subgraph(A3LNetwork, vids = all_proteins)
A3LNetwork_prot %>% summary
# Extract clusters from the PPI network
A3LNetwork_prot_clust = extract_cluster(Graph = A3LNetwork_prot, start_nodes = all_proteins, only_cluster = TRUE)
Cluster_list_gene = lapply(A3LNetwork_prot_clust, function(clust) suppressMessages(unname(AnnotationDbi::mapIds(org.Hs.eg.db, str_remove(clust, "Prot_"),"SYMBOL","UNIPROT"))))
A3LNetwork_prot_clust = A3LNetwork_prot_clust[nbrs(A3LNetwork_prot_clust) > 10] # Results in 55 modules
# Convert protein clusters to gene clusters.
Cluster_list_gene = lapply(A3LNetwork_prot_clust, function(clust) suppressMessages(unname(AnnotationDbi::mapIds(org.Hs.eg.db, str_remove(clust, "Prot_"),"SYMBOL","UNIPROT"))))
Cluster_list_gene = lapply(Cluster_list_gene, function(x) paste0("Gene_", x))
A3LNetwork_prot_clust %>% nbr
Cluster_list_gene %>% nbr
Cluster_list_gene %>% nbrs
data_prostate_cancer = list(Targets = Known_Target_prot, Signatures = Signature_for_prostate_cancer, Clusters = A3LNetwork_prot_clust, Clusters_gene = Cluster_list_gene)
usethis::use_data(data_prostate_cancer, overwrite = TRUE)
devtools::load_all(".")
devtools::document()
# You'll need these four libraries to run this vignette, they can be be easily installed like so:
# if (!require("BiocManager", quietly = TRUE))
#   install.packages("BiocManager")
#
# BiocManager::install("org.Hs.eg.db")
# BiocManager::install("devtools")
# BiocManager::install("tidyverse")
# BiocManager::install("igraph")
# org.Hs.eg.db is optionnal
# library(org.Hs.eg.db)
library(DiscoNet)
library(tidyverse)
library(igraph)
A3LNetwork = DiscoNet::A3LNetwork
summary(A3LNetwork)
table(V(A3LNetwork)$type)
# IGRAPH 5f51ddf DN-B 7538 53919 --
# + attr: name (v/c), type (v/c)
#
#    gene      go protein
#    3757    1916    1865
# Get all proteins beginning with Prot_
all_proteins = vnames(A3LNetwork, pattern = "Prot_")
# Calculate topological features for the 1 865 proteins in the network
TopologicalMetrics = extract_topolo(A3LNetwork, all_proteins)
dim(TopologicalMetrics)
# Calculate for the 1 865 proteins their random walk distance to every other nodes
# This will take a bit of time, especially without nCores = 1 (no parallel computing)
rwr_dist = extract_by_shp(Graph = A3LNetwork, start_nodes = all_proteins)
dim(rwr_dist)
# Based on precalculated clusters done on A3LNetwork
Protein_clusters = DiscoNet::data_prostate_cancer$Clusters
# Module-based features are then calculated, including if the a node is in a module, and its distance to each module. These distances can be calculated within extract_cluster, but as we already calculated distances in the previous chunks, we can reuse these distance matrices for quicker calculation. Different distance matrices will give different results.
Dist_cluster = extract_cluster(A3LNetwork, all_proteins, dist_matrix = rwr_dist, Cluster_list = Protein_clusters)
dim(Dist_cluster)
# Clusters should be converted to the same ID than the gene layer ID
Gene_clusters = DiscoNet::data_prostate_cancer$Clusters_gene
# Convert modules to gene symbols
Gene_clusters = lapply(Gene_clusters, function(x) intersect(x, vnames(A3LNetwork)))
Gene_clusters %>% nbrs
Gene_clusters %>% nbr
# Get preloaded gene signatures for prostate cancer, it contains a list of signatures and also known therapeutic targets.
data_prostate_cancer = DiscoNet::data_prostate_cancer
# Remove genes not present in the network (it is rather a small network), resulting in 25 mutated genes and 5 differentially expressed genes.
Signature_for_prostate_cancer = lapply(data_prostate_cancer$Signatures, function(x) intersect(x, vnames(A3LNetwork)))
# Extract signature-based features for 1 865 proteins
Dist_signature = extract_by_sig(A3LNetwork, all_proteins, Signature_for_prostate_cancer, Gene_clusters, rwr_dist)
Dist_signature %>% dim
# merge all feature datasets together.
Full_features = purrr::reduce(.x = list(TopologicalMetrics, rwr_dist, Dist_cluster, Dist_signature),
merge, by = c('Target'), all = T)
Full_features %>% dim
# 15 249 is a lot of features, so variable selection will be done in order to keep only the most informative ones.
# To run information gain variable selection, a class must be given.
# In this vignette, the class will be based on whether or not a protein is a known therapeutic target for prostate cancer
# data_prostate_cancer$Targets contains 13 known targets. The other proteins are therefore assumed negatives.
class = Full_features$Target %in% data_prostate_cancer$Targets
# Create a training set and a test set
# training set contains all positive observations and a random selection of 200 negative observations on which to train
set.seed(123)
index = c(which(class),sample(which(!class), 300))
train.x = Full_features[index, -1]
train.y = class[index]
test.x = Full_features[-index, -1]
test.y = class[-index]
# Run information gain on different bootstraps
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 1)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
# BiocManager::install("kknn")
library("kknn")
# Using the k-nearest neighbors algorithm to train of these features on the training set.
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
# Predict new potential targets on the bigger test set.
PotentialNewTarget = Full_features$Target[-index][predict(KNN_mod, test.x[, best_features]) == TRUE]
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
conf_mat
infgain_full
Dist_cluster
Dist_signature
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 8,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 9,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 6,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
A3LNetwork = DiscoNet::A3LNetwork
summary(A3LNetwork)
table(V(A3LNetwork)$type)
# IGRAPH 5f51ddf DN-B 7538 53919 --
# + attr: name (v/c), type (v/c)
#
#    gene      go protein
#    3757    1916    1865
# Get all proteins beginning with Prot_
all_proteins = vnames(A3LNetwork, pattern = "Prot_")
# Calculate topological features for the 1 865 proteins in the network
TopologicalMetrics = extract_topolo(A3LNetwork, all_proteins)
dim(TopologicalMetrics)
# Calculate for the 1 865 proteins their random walk distance to every other nodes
# This will take a bit of time, especially without nCores = 1 (no parallel computing)
rwr_dist = extract_by_shp(Graph = A3LNetwork, start_nodes = all_proteins)
dim(rwr_dist)
rwr_dist = extract_by_rwr(Graph = A3LNetwork, start_nodes = all_proteins, nCores = 1)
dim(rwr_dist)
# Based on precalculated clusters done on A3LNetwork
Protein_clusters = DiscoNet::data_prostate_cancer$Clusters
# They can also be calculated using the following code
# # Create PPI subgraph
# A3LNetwork_prot =igraph::induced_subgraph(A3LNetwork, vids = all_proteins)
# # Calculate clusters, optionnaly, you don't have to calculate clusters if you have precalculated modules you want to use.
# A3LNetwork_prot_clust = extract_cluster(Graph = A3LNetwork_prot, start_nodes = all_proteins, only_cluster = TRUE)
# # Only take clusters of node size higher than 10
# A3LNetwork_prot_clust = A3LNetwork_prot_clust[nbrs(A3LNetwork_prot_clust) > 10] # Results in 55 modules
# Module-based features are then calculated, including if the a node is in a module, and its distance to each module. These distances can be calculated within extract_cluster, but as we already calculated distances in the previous chunks, we can reuse these distance matrices for quicker calculation. Different distance matrices will give different results.
Dist_cluster = extract_cluster(A3LNetwork, all_proteins, dist_matrix = rwr_dist, Cluster_list = Protein_clusters)
dim(Dist_cluster)
# Clusters should be converted to the same ID than the gene layer ID
Gene_clusters = DiscoNet::data_prostate_cancer$Clusters_gene
# Remove genes not present in the network, or else an error will be returned
Gene_clusters = lapply(Gene_clusters, function(x) intersect(x, vnames(A3LNetwork)))
# Get preloaded gene signatures for prostate cancer, it contains a list of signatures and also known therapeutic targets.
data_prostate_cancer = DiscoNet::data_prostate_cancer
# Remove genes not present in the network (it is rather a small network), resulting in 25 mutated genes and 5 differentially expressed genes.
Signature_for_prostate_cancer = lapply(data_prostate_cancer$Signatures, function(x) intersect(x, vnames(A3LNetwork)))
# Extract signature-based features for 1 865 proteins
Dist_signature = extract_by_sig(A3LNetwork, all_proteins, Signature_for_prostate_cancer, Gene_clusters, rwr_dist)
Dist_signature %>% dim
# merge all feature datasets together.
Full_features = purrr::reduce(.x = list(TopologicalMetrics, rwr_dist, Dist_cluster, Dist_signature),
merge, by = c('Target'), all = T)
Full_features %>% dim
# That is a lot of features, so variable selection will be done in order to keep only the most informative ones.
# To run information gain variable selection, a class must be given.
# In this vignette, the class will be based on whether or not a protein is a known therapeutic target for prostate cancer
# data_prostate_cancer$Targets contains 13 known targets. The other proteins are therefore assumed negatives.
class = Full_features$Target %in% data_prostate_cancer$Targets
# Create a training set and a test set
# training set contains all positive observations and a random selection of 200 negative observations on which to train
set.seed(123)
index = c(which(class),sample(which(!class), 300))
train.x = Full_features[index, -1]
train.y = class[index]
test.x = Full_features[-index, -1]
test.y = class[-index]
# Run information gain on different bootstraps
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
# Using your favorite machine learning algorithms, you can now use these features to predict new therapeutic targets for prostate cancer
# Example:::
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
if (!require("kknn", quietly = TRUE))
BiocManager::install("kknn")
library("kknn")
# Using the k-nearest neighbors algorithm to train of these features on the training set.
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 6,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
# Predict new potential targets on the bigger test set.
PotentialNewTarget = Full_features$Target[-index][predict(KNN_mod, test.x[, best_features]) == TRUE]
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 6,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 8,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 9,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 6,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 5,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 10,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 11,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 4,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 11,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
conf_mat
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 11,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.6)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 2,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
best_features2 = best_features
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
best_features2 %>% head
best_features %>% head
best_features3 = best_features
set.seed(123)
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3, seed = 123)
best_features3 %>% head
best_features2 %>% head
best_features %>% head
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
best_features3 %>% head
best_features2 %>% head
best_features %>% head
best_features2 = best_features
# Run information gain on different bootstraps
infgain_full = InformationGain_Bootstrap(df = train.x, class = train.y, nbr_of_boot = 3, seed = 123)
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:300]
best_features3 %>% head
best_features2 %>% head
best_features %>% head
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 3,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 5,  distance = 1.5)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 5,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 7,  distance = 1.8)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 5,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features[1:100]]),  ks = 5,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features[1:50]]),  ks = 5,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
# Take the best 100 features (with the highest information gain)
best_features = infgain_full$feature[1:100]
# Using the k-nearest neighbors algorithm to train of these features on the training set.
set.seed(123)
KNN_mod = kknn::train.kknn(Y~., data = data.frame(Y = as.factor(train.y), train.x[, best_features]),  ks = 5,  distance = 2)
conf_mat = table(preds = KNN_mod$fitted.values[[1]], actuals = KNN_mod$data$Y)
print(paste0("Accuracy: ", round(sum(diag(conf_mat))/sum(conf_mat), 2)))
print(paste0("Sensibility: ", round(conf_mat[1, 1]/sum(conf_mat[1, ]), 2)))
print(paste0("Sensitivity: ", round(conf_mat[2, 2]/sum(conf_mat[2, ]), 2)))
# Predict new potential targets on the bigger test set.
PotentialNewTarget = Full_features$Target[-index][predict(KNN_mod, test.x[, best_features]) == TRUE]
devtools::load_all(".")
devtools::document()
library(DiscoNet)
extract_cluster
devtools::load_all(".")
devtools::document()
library(DiscoNet)
getwd()
devtools::load_all(".")
devtools::document()
library(DiscoNet)
